{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb94c659",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9863f28",
   "metadata": {},
   "source": [
    "Data ingestion -> Document Store (Azure AI Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678408da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88d6b3",
   "metadata": {},
   "source": [
    "## 1. Ingest pdf(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd6f42",
   "metadata": {},
   "source": [
    "Ingest pdf(s) in `/data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62bd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_pdf_to_base64(file_path):\n",
    "    \"\"\"\n",
    "    Reads a PDF file and converts it to a base64 data URI.\n",
    "    Required because Azure MaaS endpoints usually don't accept local paths.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        encoded_string = base64.b64encode(pdf_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Mistral expects this exact format prefix\n",
    "    return f\"data:application/pdf;base64,{encoded_string}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3cd28",
   "metadata": {},
   "source": [
    "## 2. Run OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50950fa8",
   "metadata": {},
   "source": [
    "Run OCR to extract text from each page. Mistral document model (https://docs.mistral.ai/capabilities/document_ai), it is on Azure AI foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f5ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDFs. Starting OCR job...\n",
      "\n",
      "Processing: interferometric_single-shot_parity_measurement.pdf... Done.\n",
      "Processing: optimizing_pairwise_measurement-based_surface_code.pdf... Done.\n",
      "Processing: qkd-chemistry_a_modular_toolkit_for_quantum_chemistry_applications.pdf... Done.\n",
      "Processing: roadmap_to_fault_tolerant_quantum_computation_using_topological_qubit_arrays.pdf... Done.\n",
      "\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import base64\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "results = []\n",
    "pdf_files = glob.glob(os.path.join(\"data\", \"*.pdf\"))\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDFs. Starting OCR job...\\n\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('AZURE_OPENAI_API_KEY')}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Pages per batch request (to avoid azure timeout for large PDFs)\n",
    "BATCH_SIZE = 5 \n",
    "\n",
    "for file_path in pdf_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing: {file_name}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Split pdf into chunks/batches\n",
    "        reader = PdfReader(file_path)\n",
    "        total_pages = len(reader.pages)\n",
    "        file_page_data = [] # Store all pages for this file here\n",
    "\n",
    "        # Iterate in batches (e.g., 0-5, 5-10, etc.)\n",
    "        for start_idx in range(0, total_pages, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, total_pages)\n",
    "            \n",
    "            # Create a temporary PDF in memory for this batch\n",
    "            writer = PdfWriter()\n",
    "            for i in range(start_idx, end_idx):\n",
    "                writer.add_page(reader.pages[i])\n",
    "            \n",
    "            with io.BytesIO() as bytes_stream:\n",
    "                writer.write(bytes_stream)\n",
    "                bytes_stream.seek(0)\n",
    "                encoded_batch = base64.b64encode(bytes_stream.read()).decode(\"utf-8\")\n",
    "                base64_string = f\"data:application/pdf;base64,{encoded_batch}\"\n",
    "\n",
    "            # 1. Prepare Payload (using the batch instead of full file)\n",
    "            payload = {\n",
    "                \"model\": \"mistral-document-ai-2512\",\n",
    "                \"document\": {\n",
    "                    \"type\": \"document_url\",\n",
    "                    \"document_url\": base64_string\n",
    "                },\n",
    "                \"include_image_base64\": False \n",
    "            }\n",
    "            \n",
    "            # 2. Send Request\n",
    "            response = requests.post(os.getenv(\"AZURE_MISTRAL_ENDPOINT\"), headers=headers, json=payload)\n",
    "            \n",
    "            # 3. Handle Response\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                # Combine this batch's pages into the main list\n",
    "                for i, page in enumerate(data.get('pages', [])):\n",
    "                    file_page_data.append({\n",
    "                        \"page_num\": start_idx + i + 1,  # Calculate correct page number\n",
    "                        \"text\": page['markdown']\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"\\nError on batch {start_idx}-{end_idx}: {response.status_code} - {response.text}\")\n",
    "                break # Stop processing this file if a batch fails\n",
    "        \n",
    "        # Only append if we got data\n",
    "        if file_page_data:\n",
    "            results.append({\n",
    "                \"source_context\": file_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"pages\": file_page_data \n",
    "            })\n",
    "            print(\"Done.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727842ce",
   "metadata": {},
   "source": [
    "## 3. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68974c8c",
   "metadata": {},
   "source": [
    "Two-stage markdown-aware chunking strategy:\n",
    "1. **`MarkdownHeaderTextSplitter`** — splits by section headers (`#`, `##`, `###`) and preserves header hierarchy as metadata. This keeps semantically related content together, respecting the structure of research papers.\n",
    "2. **`RecursiveCharacterTextSplitter`** — second pass to enforce size limits on chunks that are still too large after header-based splitting.\n",
    "\n",
    "Since Mistral OCR outputs markdown, this approach preserves section boundaries, tables, and equations far better than a generic character splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff2233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Pass 1: Split by markdown headers (preserves section context as metadata)\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"section\"),\n",
    "    (\"##\", \"subsection\"),\n",
    "    (\"###\", \"subsubsection\"),\n",
    "]\n",
    "md_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False  # Keep headers in the chunk text for better search relevance\n",
    ")\n",
    "\n",
    "# Pass 2: Enforce size limits on chunks that are still too large\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=500,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a616e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 299 chunks with page numbers and section metadata.\n",
      "Sample sections: {'Interferometric Single-Shot Parity Measurement in InAs-Al Hybrid Devices > 2 Topological qubit device design and setup', 'Interferometric Single-Shot Parity Measurement in InAs-Al Hybrid Devices', 'Interferometric Single-Shot Parity Measurement in InAs-Al Hybrid Devices > 1 Introduction', '3. FERMION PARITY MEASUREMENT AND INTERPRETATION', 'Unknown Section'}\n"
     ]
    }
   ],
   "source": [
    "chunked_data = []\n",
    "\n",
    "for doc in results:\n",
    "    filename = doc['source_context']\n",
    "    \n",
    "    for page in doc['pages']:\n",
    "        page_num = page['page_num']\n",
    "        page_text = page['text']\n",
    "        \n",
    "        # Stage 1: Split by markdown headers\n",
    "        md_chunks = md_splitter.split_text(page_text)\n",
    "        \n",
    "        # Stage 2: Enforce size limits on each header-based chunk\n",
    "        final_chunks = text_splitter.split_documents(md_chunks)\n",
    "        \n",
    "        for i, chunk in enumerate(final_chunks):\n",
    "            # Build section path from header metadata (e.g. \"Introduction > Device Design\")\n",
    "            section_parts = []\n",
    "            for key in [\"section\", \"subsection\", \"subsubsection\"]:\n",
    "                if key in chunk.metadata:\n",
    "                    section_parts.append(chunk.metadata[key])\n",
    "            section_path = \" > \".join(section_parts) if section_parts else \"Unknown Section\"\n",
    "            \n",
    "            chunked_data.append({\n",
    "                \"chunk_id\": f\"{filename}_p{page_num}_{i}\",\n",
    "                \"source\": filename,\n",
    "                \"page\": page_num,\n",
    "                \"section\": section_path,\n",
    "                \"text\": chunk.page_content\n",
    "            })\n",
    "\n",
    "print(f\"Generated {len(chunked_data)} chunks with page numbers and section metadata.\")\n",
    "print(f\"Sample sections: {set(c['section'] for c in chunked_data[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd076592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk from interferometric_single-shot_parity_measurement.pdf\n",
      "# Interferometric Single-Shot Parity Measurement in InAs-Al Hybrid Devices  \n",
      "Microsoft Azure Quantum [ ]  \n",
      "###### Abstract  \n",
      "The fusion of non-Abelian...\n",
      "\n",
      "\n",
      "Chunk from interferometric_single-shot_parity_measurement.pdf\n",
      "## 1 Introduction  \n",
      "In order to leverage a topological phase for quantum computation, it is crucial to manipulate and measure the topological charge. ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the first 2 chunks\n",
    "for chunk in chunked_data[:2]:\n",
    "    print(f\"Chunk from {chunk['source']}\")\n",
    "    print(chunk['text'][:150] + \"...\") # Print first 150 chars\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0e7dd",
   "metadata": {},
   "source": [
    "## 4. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a187b",
   "metadata": {},
   "source": [
    "Generate vector embeddings per chunk using the Azure OpenAI embedding model. (https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/embeddings?view=foundry-classic&tabs=csharp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e7cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 299 chunks...\n",
      "..............................\n",
      "Done! Embeddings generated.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Setup Client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \") # Clean newlines to avoid token weirdness\n",
    "    return client.embeddings.create(\n",
    "        input=[text], \n",
    "        model=\"text-embedding-3-large\",\n",
    "    ).data[0].embedding\n",
    "\n",
    "# Apply to all chunks\n",
    "print(f\"Embedding {len(chunked_data)} chunks...\")\n",
    "\n",
    "for i, chunk in enumerate(chunked_data):\n",
    "    try:\n",
    "        vector = get_embedding(chunk['text'])\n",
    "        chunk['values'] = vector # Store the 3072 float list\n",
    "        \n",
    "        if i % 10 == 0: print(f\".\", end=\"\") # Progress bar\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on chunk {i}: {e}\")\n",
    "\n",
    "print(\"\\nDone! Embeddings generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ae95e",
   "metadata": {},
   "source": [
    "## 5. Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ef5cd",
   "metadata": {},
   "source": [
    "Index in Azure AI Search: store chunk text + metadata (document id, page number, folder, category, source_link) + embedding vector; enable vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800f2a7",
   "metadata": {},
   "source": [
    "get data ready for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a860a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payload from 299 chunks...\n",
      "Ready to upload 299 documents.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "documents_to_upload = []\n",
    "\n",
    "print(f\"Preparing payload from {len(chunked_data)} chunks...\")\n",
    "\n",
    "for chunk in chunked_data:\n",
    "    # Include section info in location for richer citations\n",
    "    section_info = f\", Section: {chunk['section']}\" if chunk.get('section') and chunk['section'] != \"Unknown Section\" else \"\"\n",
    "    \n",
    "    azure_doc = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": chunk['text'],\n",
    "        \"contentVector\": chunk['values'],\n",
    "        \"location\": f\"Source: {chunk['source']} (Page {chunk['page']}{section_info})\" \n",
    "    }\n",
    "    \n",
    "    documents_to_upload.append(azure_doc)\n",
    "\n",
    "print(f\"Ready to upload {len(documents_to_upload)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e846af9",
   "metadata": {},
   "source": [
    "upload to AI search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed396c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded batch 0 - 299: Success\n",
      "Upload Complete.\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Initialize Client\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_PRIMARY_API_KEY\"))\n",
    "client = SearchClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "                      index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "                      credential=credential)\n",
    "\n",
    "# Upload in batches (Azure has a limit of ~1000 docs per request)\n",
    "BATCH_SIZE = 1000\n",
    "for i in range(0, len(documents_to_upload), BATCH_SIZE):\n",
    "    batch = documents_to_upload[i : i + BATCH_SIZE]\n",
    "    \n",
    "    try:\n",
    "        result = client.upload_documents(documents=batch)\n",
    "        print(f\"Uploaded batch {i} - {i+len(batch)}: Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading batch {i}: {e}\")\n",
    "\n",
    "print(\"Upload Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1cd30",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbfc8a",
   "metadata": {},
   "source": [
    "Validate end-to-end: run a few test queries, confirm top results point back to the right page/chunk, and iterate on chunking/cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a58ca",
   "metadata": {},
   "source": [
    "without mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8301eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating query embedding... Done.\n",
      "Searching Vector Index... Found 3 relevant chunks.\n",
      "\n",
      "Answer:\n",
      "The QDK/Chemistry toolkit uses a **factory-based interface** to enable seamless swapping of algorithm backends (e.g., switching to PySCF) without modifying your main Python workflow. Here’s how it works:\n",
      "\n",
      "1. **Algorithm Interfaces and Factories**:\n",
      "   Each algorithm type (e.g., a quantum chemistry solver) has a **common interface** defining its input/output requirements. A **factory** maintains a registry of available implementations (e.g., PySCF, Psi4, or custom backends) for that interface. When you request an implementation by name (e.g., `\"PySCF\"`), the factory returns an object conforming to the interface, abstracting the concrete backend from your workflow [Source: qkd-chemistry_a_modular_toolkit_for_quantum_chemistry_applications.pdf, Page: 6].\n",
      "\n",
      "2. **Transparent Substitution**:\n",
      "   Your client code interacts with the **interface**, not the specific backend. This means you can switch implementations (e.g., from a default solver to PySCF) by simply changing the name passed to the factory, without rewriting the surrounding logic [Source: qkd-chemistry_a_modular_toolkit_for_quantum_chemistry_applications.pdf, Page: 6].\n",
      "\n",
      "3. **Runtime Discoverability**:\n",
      "   You can query the factory at runtime to list available backends (e.g., `factory.list_implementations()`), making it easy to explore alternatives dynamically [Source: qkd-chemistry_a_modular_toolkit_for_quantum_chemistry_applications.pdf, Page: 6].\n",
      "\n",
      "4. **Multi-Language Support**:\n",
      "   QDK/Chemistry’s Python bindings (via `pybind11`) expose the same API as the native C++ implementations, allowing you to integrate backends like PySCF (which itself uses Python-driven workflows) without language barriers. This ensures compatibility with existing tools while maintaining performance [Source: qkd-chemistry_a_modular_toolkit_for_quantum_chemistry_applications.pdf, Page: 8].\n",
      "\n",
      "### Example Workflow:\n",
      "```python\n",
      "# Request an implementation by name (e.g., \"PySCF\")\n",
      "solver = factory.create(\"PySCF\")  # Returns an object conforming to the interface\n",
      "result = solver.run(molecule)     # Your workflow remains unchanged\n",
      "```\n",
      "By decoupling interfaces from implementations, QDK/Chemistry lets you swap backends (like PySCF) without touching the core logic.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import sys\n",
    "\n",
    "# Embedding Client (Azure OpenAI - for converting query to vector)\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# 2. Search Client (Azure AI Search - for finding relevant docs)\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_SEARCH_API_KEY\")) # this is using the query key, use primary key if it doesnt work\n",
    ")\n",
    "\n",
    "# 3. Chat Client (Mistral on Azure MaaS)\n",
    "chat_client = OpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_INFERENCE\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def retrieve_context(query_text):\n",
    "    print(\"Generating query embedding...\", end=\" \")\n",
    "    # Generate Vector for the user's query\n",
    "    embedding_response = embedding_client.embeddings.create(\n",
    "        input=query_text,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    query_vector = embedding_response.data[0].embedding\n",
    "    print(\"Done.\")\n",
    "\n",
    "    print(\"Searching Vector Index...\", end=\" \")\n",
    "    # Perform Vector Search\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector, \n",
    "        k_nearest_neighbors=3, \n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "    \n",
    "    results = search_client.search(\n",
    "        search_text=query_text, # Hybrid search (keywords + vector)\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"content\", \"location\"],\n",
    "        top=3\n",
    "    )\n",
    "\n",
    "    # Format results as a single string\n",
    "    context_parts = []\n",
    "    for result in results:\n",
    "        context_parts.append(f\"Source: {result['location']}\\nContent: {result['content']}\")\n",
    "    \n",
    "    print(f\"Found {len(context_parts)} relevant chunks.\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# Main\n",
    "user_input = input(\"\\nWhat would you like to know? (leave empty if you want to select from predefined test queries)\")\n",
    "\n",
    "# use predefined test query\n",
    "if user_input == \"\":\n",
    "    no_input_question = input(\"[1] What modular software toolkit is introduced to connect classical electronic structure calculations to quantum circuit execution? \\n[2] In the newly proposed pairwise measurement-based surface code, what is the exact fault-tolerance threshold achieved under a standard circuit noise model? \\n[3] For the single-qubit tetron device, how do the \\\"detuning-based\\\" and \\\"cutter-based\\\" approaches differ in decoupling quantum dots from the qubit island, and how does each approach specifically affect residual coupling and overall qubit coherence?\")\n",
    "    \n",
    "    if no_input_question == \"1\":\n",
    "        user_input = \"What modular software toolkit is introduced to connect classical electronic structure calculations to quantum circuit execution?\"\n",
    "    elif no_input_question == \"2\":\n",
    "        user_input = \"In the newly proposed pairwise measurement-based surface code, what is the exact fault-tolerance threshold achieved under a standard circuit noise model?\"\n",
    "    elif no_input_question == \"3\":\n",
    "        user_input = \"For the single-qubit tetron device, how do the \\\"detuning-based\\\" and \\\"cutter-based\\\" approaches differ in decoupling quantum dots from the qubit island, and how does each approach specifically affect residual coupling and overall qubit coherence?\"\n",
    "    else:\n",
    "        print(\"no query given\")\n",
    "        sys.exit()\n",
    "\n",
    "# Get relevant context\n",
    "retrieved_context = retrieve_context(user_input)\n",
    "\n",
    "# Define System Prompt\n",
    "system_prompt = \"\"\"You are a helpful assistant. Use the provided 'Context' to answer the user's question.\n",
    "If the answer is not in the context, say you don't know.\n",
    "Always cite your sources using the format [Source: filename, Page: page].\"\"\"\n",
    "\n",
    "# Call Mistral with Context\n",
    "completion = chat_client.chat.completions.create(\n",
    "    model=\"Mistral-Large-3\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Context:\\n{retrieved_context}\\n\\nQuestion: {user_input}\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9778f",
   "metadata": {},
   "source": [
    "## 7. Use AI Search MCP on GHCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55899af",
   "metadata": {},
   "source": [
    "We have bult a custom MCP server for Azure AI Search retrieval. This will be needed for vector/hybrid search for the agent, this is custom built in python using FastMCP, see [`./azure-ai-search-mcp`](./azure-ai-search-mcp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11df617",
   "metadata": {},
   "source": [
    "## 8. Integrate MCP with OpenWebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee545dc7",
   "metadata": {},
   "source": [
    "OpenWebUI doesn't support stdio MCP configurations natively, use `mcpo` python library for it to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18088ddc",
   "metadata": {},
   "source": [
    "OpenWebUI locally w/ uv + python: https://docs.openwebui.com/getting-started/quick-start/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875c6bc",
   "metadata": {},
   "source": [
    "Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0471f7",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt # contains open-webui dep already\n",
    "\n",
    "# manually install if you want:\n",
    "pip install open-webui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9636d9",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c390f",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "$env:DATA_DIR=\"C:\\open-webui\\data\"; open-webui serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005dcde",
   "metadata": {},
   "source": [
    "Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3927f62",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U open-webui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5449880",
   "metadata": {},
   "source": [
    "Uninstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bebd0",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "uv tool uninstall open-webui\n",
    "uv cache clean\n",
    "\n",
    "# DELETE ALL DATA\n",
    "rm -rf ~/.open-webui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
