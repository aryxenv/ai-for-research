{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb94c659",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9863f28",
   "metadata": {},
   "source": [
    "Data ingestion -> Document Store (Azure AI Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88d6b3",
   "metadata": {},
   "source": [
    "## 1. Ingest pdf(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd6f42",
   "metadata": {},
   "source": [
    "Ingest pdf(s) in `/data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62bd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_pdf_to_base64(file_path):\n",
    "    \"\"\"\n",
    "    Reads a PDF file and converts it to a base64 data URI.\n",
    "    Required because Azure MaaS endpoints usually don't accept local paths.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        encoded_string = base64.b64encode(pdf_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Mistral expects this exact format prefix\n",
    "    return f\"data:application/pdf;base64,{encoded_string}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3cd28",
   "metadata": {},
   "source": [
    "## 2. Run OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50950fa8",
   "metadata": {},
   "source": [
    "Run OCR to extract text from each page. Mistral document model (https://docs.mistral.ai/capabilities/document_ai), it is on Azure AI foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDFs. Starting OCR job...\n",
      "\n",
      "Processing: embedding_retrieval.pdf... Failed: Expecting value: line 1 column 1 (char 0)\n",
      "Processing: refrag_research.pdf... Failed: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "# currently broken\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import dotenv\n",
    "import requests\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "results = []\n",
    "pdf_files = glob.glob(os.path.join(\"data\", \"*.pdf\"))\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDFs. Starting OCR job...\\n\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('AZURE_OPENAI_API_KEY')}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing: {file_name}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # 1. Prepare Payload\n",
    "        payload = {\n",
    "            \"model\": \"mistral-document-ai-2505\",\n",
    "            \"document\": {\n",
    "                \"type\": \"document_url\",\n",
    "                \"document_url\": encode_pdf_to_base64(file_path)\n",
    "            },\n",
    "            \"include_image_base64\": False # optionally set to true\n",
    "        }\n",
    "        \n",
    "        # 2. Send Request\n",
    "        response = requests.post(os.getenv(\"AZURE_OPENAI_ENDPOINT\"), headers=headers, json=payload)\n",
    "        \n",
    "        # 3. Handle Response\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # preserve page location\n",
    "            page_data = []\n",
    "            for i, page in enumerate(data.get('pages', [])):\n",
    "                page_data.append({\n",
    "                    \"page_num\": i + 1,  # Humans start at 1\n",
    "                    \"text\": page['markdown']\n",
    "                })\n",
    "            \n",
    "            results.append({\n",
    "                \"source_context\": file_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"pages\": page_data # Store the list, not the string\n",
    "            })\n",
    "            print(\"Done.\")\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727842ce",
   "metadata": {},
   "source": [
    "## 3. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68974c8c",
   "metadata": {},
   "source": [
    "Chunk OCR test with a simple simple textsplitter (https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#langchain-data-chunking-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-text-splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure the splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,   # Characters per chunk (adjust based on your embedding model limit)\n",
    "    chunk_overlap=500, # overlap keeps context between cuts\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Try to split by paragraphs first, then lines, then words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a616e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "\n",
    "# iterating over the 'results' list from the previous OCR step\n",
    "for doc in results:\n",
    "    filename = doc['source_context']\n",
    "    \n",
    "    # Iterate through each PAGE first\n",
    "    for page in doc['pages']:\n",
    "        page_num = page['page_num']\n",
    "        page_text = page['text']\n",
    "        \n",
    "        # Split ONLY this page's text\n",
    "        chunks = splitter.split_text(page_text)\n",
    "        \n",
    "        for i, text_chunk in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                \"chunk_id\": f\"{filename}_p{page_num}_{i}\",\n",
    "                \"source\": filename,\n",
    "                \"page\": page_num,\n",
    "                \"text\": text_chunk\n",
    "            })\n",
    "\n",
    "print(f\"Generated {len(chunked_data)} chunks with page numbers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd076592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 2 chunks\n",
    "for chunk in chunked_data[:2]:\n",
    "    print(f\"Chunk from {chunk['source']}\")\n",
    "    print(chunk['text'][:150] + \"...\") # Print first 150 chars\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0e7dd",
   "metadata": {},
   "source": [
    "## 4. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a187b",
   "metadata": {},
   "source": [
    "Generate vector embeddings per chunk using the Azure OpenAI embedding model. (https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/embeddings?view=foundry-classic&tabs=csharp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Setup Client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \") # Clean newlines to avoid token weirdness\n",
    "    return client.embeddings.create(\n",
    "        input=[text], \n",
    "        model=\"text-embedding-3-large\",\n",
    "    ).data[0].embedding\n",
    "\n",
    "# Apply to all chunks\n",
    "print(f\"Embedding {len(chunked_data)} chunks...\")\n",
    "\n",
    "for i, chunk in enumerate(chunked_data):\n",
    "    try:\n",
    "        vector = get_embedding(chunk['text'])\n",
    "        chunk['values'] = vector # Store the 3072 float list\n",
    "        \n",
    "        if i % 10 == 0: print(f\".\", end=\"\") # Progress bar\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on chunk {i}: {e}\")\n",
    "\n",
    "print(\"\\nDone! Embeddings generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ae95e",
   "metadata": {},
   "source": [
    "## 5. Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ef5cd",
   "metadata": {},
   "source": [
    "Index in Azure AI Search: store chunk text + metadata (document id, page number, folder, category, source_link) + embedding vector; enable vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800f2a7",
   "metadata": {},
   "source": [
    "get data ready for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a860a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "documents_to_upload = []\n",
    "\n",
    "print(f\"Preparing payload from {len(chunked_data)} chunks...\")\n",
    "\n",
    "for chunk in chunked_data:\n",
    "    # 1. Extract context (We only have filename now, not page number)\n",
    "    filename = chunk['source']\n",
    "    \n",
    "    # 2. Map to your Azure Search Index Schema\n",
    "    azure_doc = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": chunk['text'],\n",
    "        \"contentVector\": chunk['values'],\n",
    "        # Citation will look like: \"Source: report.pdf (Page 4)\"\n",
    "        \"location\": f\"Source: {chunk['source']} (Page {chunk['page']})\" \n",
    "    }\n",
    "    \n",
    "    documents_to_upload.append(azure_doc)\n",
    "\n",
    "print(f\"Ready to upload {len(documents_to_upload)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e846af9",
   "metadata": {},
   "source": [
    "upload to AI search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed396c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Initialize Client\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_API_KEY\"))\n",
    "client = SearchClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "                      index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "                      credential=credential)\n",
    "\n",
    "# Upload in batches (Azure has a limit of ~1000 docs per request)\n",
    "BATCH_SIZE = 1000\n",
    "for i in range(0, len(documents_to_upload), BATCH_SIZE):\n",
    "    batch = documents_to_upload[i : i + BATCH_SIZE]\n",
    "    \n",
    "    try:\n",
    "        result = client.upload_documents(documents=batch)\n",
    "        print(f\"Uploaded batch {i} - {i+len(batch)}: Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading batch {i}: {e}\")\n",
    "\n",
    "print(\"Upload Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1cd30",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbfc8a",
   "metadata": {},
   "source": [
    "Validate end-to-end: run a few test queries, confirm top results point back to the right page/chunk, and iterate on chunking/cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9778f",
   "metadata": {},
   "source": [
    "## 7. Use AI Search MCP on GHCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55899af",
   "metadata": {},
   "source": [
    "This will be needed to set up an MCP server for AI search (for vector/hybrid search): https://github.com/tomgutt/azure-ai-search-mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11df617",
   "metadata": {},
   "source": [
    "## 8. Integrate MCP with OpenWebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee545dc7",
   "metadata": {},
   "source": [
    "OpenWebUI doesn't support stdio MCP configurations natively, use `mcpo` python library for it to work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
