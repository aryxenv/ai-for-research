{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb94c659",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9863f28",
   "metadata": {},
   "source": [
    "Data ingestion -> Document Store (Azure AI Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "678408da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88d6b3",
   "metadata": {},
   "source": [
    "## 1. Ingest pdf(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd6f42",
   "metadata": {},
   "source": [
    "Ingest pdf(s) in `/data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62bd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_pdf_to_base64(file_path):\n",
    "    \"\"\"\n",
    "    Reads a PDF file and converts it to a base64 data URI.\n",
    "    Required because Azure MaaS endpoints usually don't accept local paths.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        encoded_string = base64.b64encode(pdf_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Mistral expects this exact format prefix\n",
    "    return f\"data:application/pdf;base64,{encoded_string}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3cd28",
   "metadata": {},
   "source": [
    "## 2. Run OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50950fa8",
   "metadata": {},
   "source": [
    "Run OCR to extract text from each page. Mistral document model (https://docs.mistral.ai/capabilities/document_ai), it is on Azure AI foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDFs. Starting OCR job...\n",
      "\n",
      "Processing: embedding_retrieval.pdf... Done.\n",
      "Processing: refrag_research.pdf... Done.\n",
      "\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import base64\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "results = []\n",
    "pdf_files = glob.glob(os.path.join(\"data\", \"*.pdf\"))\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDFs. Starting OCR job...\\n\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('AZURE_OPENAI_API_KEY')}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Pages per batch request (to avoid azure timeout for large PDFs)\n",
    "BATCH_SIZE = 5 \n",
    "\n",
    "for file_path in pdf_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing: {file_name}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Split pdf into chunks/batches\n",
    "        reader = PdfReader(file_path)\n",
    "        total_pages = len(reader.pages)\n",
    "        file_page_data = [] # Store all pages for this file here\n",
    "\n",
    "        # Iterate in batches (e.g., 0-5, 5-10, etc.)\n",
    "        for start_idx in range(0, total_pages, BATCH_SIZE):\n",
    "            end_idx = min(start_idx + BATCH_SIZE, total_pages)\n",
    "            \n",
    "            # Create a temporary PDF in memory for this batch\n",
    "            writer = PdfWriter()\n",
    "            for i in range(start_idx, end_idx):\n",
    "                writer.add_page(reader.pages[i])\n",
    "            \n",
    "            with io.BytesIO() as bytes_stream:\n",
    "                writer.write(bytes_stream)\n",
    "                bytes_stream.seek(0)\n",
    "                encoded_batch = base64.b64encode(bytes_stream.read()).decode(\"utf-8\")\n",
    "                base64_string = f\"data:application/pdf;base64,{encoded_batch}\"\n",
    "\n",
    "            # 1. Prepare Payload (using the batch instead of full file)\n",
    "            payload = {\n",
    "                \"model\": \"mistral-document-ai-2505\",\n",
    "                \"document\": {\n",
    "                    \"type\": \"document_url\",\n",
    "                    \"document_url\": base64_string\n",
    "                },\n",
    "                \"include_image_base64\": False \n",
    "            }\n",
    "            \n",
    "            # 2. Send Request\n",
    "            response = requests.post(os.getenv(\"AZURE_MISTRAL_ENDPOINT\"), headers=headers, json=payload)\n",
    "            \n",
    "            # 3. Handle Response\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                # Combine this batch's pages into the main list\n",
    "                for i, page in enumerate(data.get('pages', [])):\n",
    "                    file_page_data.append({\n",
    "                        \"page_num\": start_idx + i + 1,  # Calculate correct page number\n",
    "                        \"text\": page['markdown']\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"\\nError on batch {start_idx}-{end_idx}: {response.status_code} - {response.text}\")\n",
    "                break # Stop processing this file if a batch fails\n",
    "        \n",
    "        # Only append if we got data\n",
    "        if file_page_data:\n",
    "            results.append({\n",
    "                \"source_context\": file_name,\n",
    "                \"file_path\": file_path,\n",
    "                \"pages\": file_page_data \n",
    "            })\n",
    "            print(\"Done.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727842ce",
   "metadata": {},
   "source": [
    "## 3. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68974c8c",
   "metadata": {},
   "source": [
    "Chunk OCR test with a simple simple textsplitter (https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#langchain-data-chunking-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ff2233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-text-splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure the splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,   # Characters per chunk (adjust based on your embedding model limit)\n",
    "    chunk_overlap=500, # overlap keeps context between cuts\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Try to split by paragraphs first, then lines, then words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a616e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 116 chunks with page numbers.\n"
     ]
    }
   ],
   "source": [
    "chunked_data = []\n",
    "\n",
    "# iterating over the 'results' list from the previous OCR step\n",
    "for doc in results:\n",
    "    filename = doc['source_context']\n",
    "    \n",
    "    # Iterate through each PAGE first\n",
    "    for page in doc['pages']:\n",
    "        page_num = page['page_num']\n",
    "        page_text = page['text']\n",
    "        \n",
    "        # Split ONLY this page's text\n",
    "        chunks = splitter.split_text(page_text)\n",
    "        \n",
    "        for i, text_chunk in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                \"chunk_id\": f\"{filename}_p{page_num}_{i}\",\n",
    "                \"source\": filename,\n",
    "                \"page\": page_num,\n",
    "                \"text\": text_chunk\n",
    "            })\n",
    "\n",
    "print(f\"Generated {len(chunked_data)} chunks with page numbers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd076592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk from embedding_retrieval.pdf\n",
      "On the Theoretical Limitations of Embedding-Based Retrieval\n",
      "\n",
      "Orion Weller^{*,1,2}, Michael Boratko^{1}, Iftekhar Naim^{1} and Jinhyuk Lee^{1}\n",
      "\n",
      "^{1}Goo...\n",
      "\n",
      "\n",
      "Chunk from embedding_retrieval.pdf\n",
      "In recent years this has been pushed even further with the rise of instruction-following retrieval benchmarks, where models are asked to represent any...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the first 2 chunks\n",
    "for chunk in chunked_data[:2]:\n",
    "    print(f\"Chunk from {chunk['source']}\")\n",
    "    print(chunk['text'][:150] + \"...\") # Print first 150 chars\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0e7dd",
   "metadata": {},
   "source": [
    "## 4. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a187b",
   "metadata": {},
   "source": [
    "Generate vector embeddings per chunk using the Azure OpenAI embedding model. (https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/embeddings?view=foundry-classic&tabs=csharp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62e7cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 116 chunks...\n",
      "............\n",
      "Done! Embeddings generated.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Setup Client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \") # Clean newlines to avoid token weirdness\n",
    "    return client.embeddings.create(\n",
    "        input=[text], \n",
    "        model=\"text-embedding-3-small\",\n",
    "    ).data[0].embedding\n",
    "\n",
    "# Apply to all chunks\n",
    "print(f\"Embedding {len(chunked_data)} chunks...\")\n",
    "\n",
    "for i, chunk in enumerate(chunked_data):\n",
    "    try:\n",
    "        vector = get_embedding(chunk['text'])\n",
    "        chunk['values'] = vector # Store the 3072 float list\n",
    "        \n",
    "        if i % 10 == 0: print(f\".\", end=\"\") # Progress bar\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on chunk {i}: {e}\")\n",
    "\n",
    "print(\"\\nDone! Embeddings generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ae95e",
   "metadata": {},
   "source": [
    "## 5. Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ef5cd",
   "metadata": {},
   "source": [
    "Index in Azure AI Search: store chunk text + metadata (document id, page number, folder, category, source_link) + embedding vector; enable vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800f2a7",
   "metadata": {},
   "source": [
    "get data ready for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a860a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payload from 116 chunks...\n",
      "Ready to upload 116 documents.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "documents_to_upload = []\n",
    "\n",
    "print(f\"Preparing payload from {len(chunked_data)} chunks...\")\n",
    "\n",
    "for chunk in chunked_data:\n",
    "    # Map to your Azure Search Index Schema\n",
    "    azure_doc = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": chunk['text'],\n",
    "        \"contentVector\": chunk['values'],\n",
    "        # Citation will look like: \"Source: report.pdf (Page 4)\"\n",
    "        \"location\": f\"Source: {chunk['source']} (Page {chunk['page']})\" \n",
    "    }\n",
    "    \n",
    "    documents_to_upload.append(azure_doc)\n",
    "\n",
    "print(f\"Ready to upload {len(documents_to_upload)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e846af9",
   "metadata": {},
   "source": [
    "upload to AI search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed396c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded batch 0 - 116: Success\n",
      "Upload Complete.\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Initialize Client\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_PRIMARY_API_KEY\"))\n",
    "client = SearchClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "                      index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "                      credential=credential)\n",
    "\n",
    "# Upload in batches (Azure has a limit of ~1000 docs per request)\n",
    "BATCH_SIZE = 1000\n",
    "for i in range(0, len(documents_to_upload), BATCH_SIZE):\n",
    "    batch = documents_to_upload[i : i + BATCH_SIZE]\n",
    "    \n",
    "    try:\n",
    "        result = client.upload_documents(documents=batch)\n",
    "        print(f\"Uploaded batch {i} - {i+len(batch)}: Success\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading batch {i}: {e}\")\n",
    "\n",
    "print(\"Upload Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1cd30",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbfc8a",
   "metadata": {},
   "source": [
    "Validate end-to-end: run a few test queries, confirm top results point back to the right page/chunk, and iterate on chunking/cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a58ca",
   "metadata": {},
   "source": [
    "without mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8301eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating query embedding... Done.\n",
      "Searching Vector Index... Found 3 relevant chunks.\n",
      "\n",
      "Answer:\n",
      "REFRAG employs curriculum learning for the reconstruction task by incrementally increasing the difficulty of the task to help the model gradually acquire complex skills. Specifically:\n",
      "\n",
      "1. **Starting Simple**: The training begins with reconstructing a single chunk. The encoder receives one chunk embedding \\(\\mathbf{c}_{1}\\) for \\(x_{1:k}\\), and the decoder reconstructs the \\(k\\) tokens using the projected chunk embedding \\(\\mathbf{e}_{1}^{\\text{cnk}}\\).\n",
      "\n",
      "2. **Gradual Complexity**: The model then progresses to reconstructing longer sequences, such as \\(x_{1:2k}\\) from \\(\\mathbf{e}_{1}^{\\text{cnk}}, \\mathbf{e}_{2}^{\\text{cnk}}\\), and so on, incrementally increasing the number of chunks.\n",
      "\n",
      "3. **Adjusting Data Mixture**: The data mixture is varied over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e., \\(L\\) chunk embeddings) [Source: refrag_research.pdf, Page: 4].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import sys\n",
    "\n",
    "# Embedding Client (Azure OpenAI - for converting query to vector)\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# 2. Search Client (Azure AI Search - for finding relevant docs)\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_SEARCH_API_KEY\")) # this is using the query key, use primary key if it doesnt work\n",
    ")\n",
    "\n",
    "# 3. Chat Client (Mistral on Azure MaaS)\n",
    "chat_client = OpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_INFERENCE\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def retrieve_context(query_text):\n",
    "    print(\"Generating query embedding...\", end=\" \")\n",
    "    # Generate Vector for the user's query\n",
    "    embedding_response = embedding_client.embeddings.create(\n",
    "        input=query_text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    query_vector = embedding_response.data[0].embedding\n",
    "    print(\"Done.\")\n",
    "\n",
    "    print(\"Searching Vector Index...\", end=\" \")\n",
    "    # Perform Vector Search\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector, \n",
    "        k_nearest_neighbors=3, \n",
    "        fields=\"contentVector\"\n",
    "    )\n",
    "    \n",
    "    results = search_client.search(\n",
    "        search_text=query_text, # Hybrid search (keywords + vector)\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"content\", \"location\"],\n",
    "        top=3\n",
    "    )\n",
    "\n",
    "    # Format results as a single string\n",
    "    context_parts = []\n",
    "    for result in results:\n",
    "        context_parts.append(f\"Source: {result['location']}\\nContent: {result['content']}\")\n",
    "    \n",
    "    print(f\"Found {len(context_parts)} relevant chunks.\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# Main\n",
    "user_input = input(\"\\nWhat would you like to know? (leave empty if you want to select from predefined test queries)\")\n",
    "\n",
    "# use predefined test query\n",
    "if user_input == \"\":\n",
    "    no_input_question = input(\"[1] What is the primary trade-off in RAG systems that REFRAG aims to solve? [2] What is the Time-To-First-Token (TTFT) acceleration achieved by REFRAG compared to LLaMA? [3] How does REFRAG use curriculum learning for the reconstruction task?\")\n",
    "    \n",
    "    if no_input_question == \"1\":\n",
    "        user_input = \"What is the primary trade-off in RAG systems that REFRAG aims to solve?\"\n",
    "    elif no_input_question == \"2\":\n",
    "        user_input = \"What is the Time-To-First-Token (TTFT) acceleration achieved by REFRAG compared to LLaMA?\"\n",
    "    elif no_input_question == \"3\":\n",
    "        user_input = \"How does REFRAG use curriculum learning for the reconstruction task?\"\n",
    "    else:\n",
    "        print(\"no query given\")\n",
    "        sys.exit()\n",
    "\n",
    "# Get relevant context\n",
    "retrieved_context = retrieve_context(user_input)\n",
    "\n",
    "# Define System Prompt\n",
    "system_prompt = \"\"\"You are a helpful assistant. Use the provided 'Context' to answer the user's question.\n",
    "If the answer is not in the context, say you don't know.\n",
    "Always cite your sources using the format [Source: filename, Page: page].\"\"\"\n",
    "\n",
    "# Call Mistral with Context\n",
    "completion = chat_client.chat.completions.create(\n",
    "    model=\"Mistral-Large-3\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Context:\\n{retrieved_context}\\n\\nQuestion: {user_input}\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9778f",
   "metadata": {},
   "source": [
    "## 7. Use AI Search MCP on GHCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55899af",
   "metadata": {},
   "source": [
    "This will be needed to set up an MCP server for AI search (for vector/hybrid search), custom built in python using FastMCP, see [`./azure-ai-search-mcp`](./azure-ai-search-mcp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11df617",
   "metadata": {},
   "source": [
    "## 8. Integrate MCP with OpenWebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee545dc7",
   "metadata": {},
   "source": [
    "OpenWebUI doesn't support stdio MCP configurations natively, use `mcpo` python library for it to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18088ddc",
   "metadata": {},
   "source": [
    "OpenWebUI locally w/ uv + python: https://docs.openwebui.com/getting-started/quick-start/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9636d9",
   "metadata": {},
   "source": [
    "Install + Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c390f",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "$env:DATA_DIR=\"C:\\open-webui\\data\"; uvx --python 3.11 open-webui@latest serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005dcde",
   "metadata": {},
   "source": [
    "Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3927f62",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U open-webui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5449880",
   "metadata": {},
   "source": [
    "Uninstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bebd0",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "uv tool uninstall open-webui\n",
    "uv cache clean\n",
    "\n",
    "# DELETE ALL DATA\n",
    "rm -rf ~/.open-webui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
